# -*- coding: utf-8 -*-
"""IT ticket classification - Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HVfIxNwMbdhvAFGvW0IBploC4ALdxTTD
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np 
import re 
import seaborn as sns
import matplotlib.pyplot as plt
from dateutil import parser
from sklearn.utils import resample
from sklearn import preprocessing

# %matplotlib inline

from nltk.corpus import stopwords
import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger') 

import spacy

from gensim.models import Word2Vec
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D,GRU,Conv1D,MaxPooling1D, TimeDistributed
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.utils import to_categorical
import tensorflow as tf
from sklearn import metrics
from tensorflow.keras import backend as K
import matplotlib.pyplot as plt
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

"""### Exploring the data"""

df = pd.read_csv('/content/drive/MyDrive/NLP/data2.csv')
df.head()

callers = df['Caller'].unique() 
print("No of callers: {0} \nCallers list: {1}".format(callers.shape, callers))
df['Description'].fillna(value=' ', inplace=True)

df.info()

"""### Visualizing data

* Find the no of clusters for each of the clusters
* Group clusters that have very few messages(less than 10) into 1
* Display the type of messages for the top 5 clusters
* Drop caller and short description as caller contains gibberish data and short description is not useful as detailed description is available
"""

no_of_clusters = df['Assignment group'].value_counts().sort_values(ascending=False).index
plt.subplots(figsize = (22,5))
ax = sns.countplot(x = 'Assignment group', data = df, color = 'blue', order = no_of_clusters)
ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = "right")
plt.tight_layout()
plt.show()

sample = df.groupby(['Assignment group'])
regroup=[]
for grp in df['Assignment group'].unique():
    if(sample.get_group(grp).shape[0]<10):
        regroup.append(grp)
print('Found {} groups which have under 10 samples'.format(len(regroup)))
df['Assignment group']=df['Assignment group'].apply(lambda x : 'misc_grp' if x in regroup  else x)

print('After regrouping: ')
no_of_clusters = df['Assignment group'].value_counts().sort_values(ascending=False).index
plt.subplots(figsize = (22,5))
ax = sns.countplot(x = 'Assignment group', data = df, color = 'red', order = no_of_clusters)
ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = "right")
plt.tight_layout()
plt.show()

no_of_clusters
for i in range(0,5,1):
    cluster_ID = no_of_clusters[i]
    for j in range(len(df)):
        if(df.iloc[j]['Assignment group'] == cluster_ID):
            print("Group ID: {0}, Type of message: {1}\n".format(cluster_ID, df.iloc[j]['Short description']))
            break

df.drop(['Caller','Short description'],axis=1,inplace= True)
df.info()

"""### Pre-processing

### 1. Cleaning the text

* Convert the text to lower case letters
* Remove email IDs
* Remove numbers
* Remove hyperlinks
* Remove encoded characters
"""

def is_valid_date(date_str):
    try:
        parser.parse(date_str)
        return True
    except:
        return False

def clean_data(text):
    text = text.lower()
    text = ' '.join([w for w in text.split() if not is_valid_date(w)])
    text = re.sub(r"received from:",' ',text)
    text = re.sub(r"from:",' ',text)
    text = re.sub(r"to:",' ',text)
    text = re.sub(r"subject:",' ',text)
    text = re.sub(r"sent:",' ',text)
    text = re.sub(r"ic:",' ',text)
    text = re.sub(r"cc:",' ',text)
    text = re.sub(r"bcc:",' ',text)
    #Remove email 
    text = re.sub(r'\S*@\S*\s?', '', text)
    # Remove numbers 
    text = re.sub(r'\d+','' ,text)
    # Remove new line characters 
    text = re.sub(r'\n',' ',text)
    # Remove hashtag while keeping hashtag text
    text = re.sub(r'#','', text)
    #& 
    text = re.sub(r'&;?', 'and',text)
    # Remove HTML special entities (e.g. &amp;)
    text = re.sub(r'\&\w*;', '', text)
    # Remove hyperlinks
    text = re.sub(r'https?:\/\/.*\/\w*', '', text)  
    # Remove characters beyond Readable formart by Unicode:
    text= ''.join(c for c in text if c <= '\uFFFF') 
    text = text.strip()
    # Remove unreadable characters  (also extra spaces)
    text = ' '.join(re.sub("[^\u0030-\u0039\u0041-\u005a\u0061-\u007a]", " ", text).split())
    for name in callers:
        namelist = [part for part in name.split()]
        for namepart in namelist: 
            text = text.replace(namepart,'')
          
    text = re.sub(r"\s+[a-zA-Z]\s+", ' ', text)
    text = re.sub(' +', ' ', text)
    text = text.strip()
    return text

df['Cleaned Description'] = df['Description'].apply(clean_data)
print("Sample Description: \n")
print("Before Cleaning: {0}\nAfter Cleaning:  {1}".format(df.iloc[1]['Description'], df.iloc[1]['Cleaned Description']))

df['Length'] = [len(text) for text in df['Cleaned Description']]
# df = df[df['Length']>=3]
# df['Cleaned Description'] = df['Cleaned Description'].apply(lambda x : " ".join([word for word in x.split() if(len(word)>2)]))

"""### 2. Remove stopwords"""

english_stopwords = stopwords.words('english')
for i in range(len(df)):
    text = df['Cleaned Description'][i]
    new_text_after_stopwords_removal = ""
    for word in text.split(' '):
        if(word not in english_stopwords):
            new_text_after_stopwords_removal += word + " "
    df['Cleaned Description'][i] = new_text_after_stopwords_removal
#     df['Cleaned Description'][i] = " ".join(word for word in text.split(' ') if word not in english_stopwords)

df.tail()

"""### Remove rows that contain description in German"""

germanwordlist = ['bitte','nicht','konto','probleme','berechtigung','defekt','mehr','ausgetauscht','rechner', 'drucker','teilweise','freigegeben','genannten','anmeldeaccount',
                  'besprochen','werden','durchwahl','oben','einrichten','zeitwirtschaft','seit','morgens','beheben','keine','zeitbuchungen','vorhanden','dringend','fehler',
                  'werk','anmelde','auftrag','kein','skannen','freundlichen','werkzeuge,','hartstoffe','maste','schutzw','fertigung','immer','sehr','zugriff','freundliche',
                  'geehrter','souzarft','noch','verbindungsherstellung','meldung','erneuten','glich','proben','beilageproben','beilage','auswerten','sinterleitstand','reparar',
                  'reparo','rechner','koenigsee','entregar','atualiza','declara','programdntya','funcionando','preciso','hitacni','grergtger','zugriffsrechte','teamleiter',
                  'abholen','wegen','weit','absender','wenn','abrechnung']
pattern = '|'.join(germanwordlist)

germanDescIndex = df[df['Cleaned Description'].str.contains(pattern)].index
print(germanDescIndex)
df.drop(df.index[[germanDescIndex]])

"""### 3. Lemmatization"""

nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
allowed_postags = ['NOUN', 'ADJ', 'VERB', 'ADV']
def lemmatize_text(text):
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc])

df['Cleaned Description'] = df['Cleaned Description'].apply(lemmatize_text)
df.head()

"""### Resampling imbalanced dataset

* From the visualization, we see that GRP_0 has high no of data (around 4k)

"""

# Create Dataset for 'others' i.e all groups which is not part of GRP_0
itTicketOthersDF = df[df['Assignment group'] != 'GRP_0']

descending_order = df['Assignment group'].value_counts().sort_values(ascending=False).index
plt.subplots(figsize=(22,5))
ax = sns.countplot(x = 'Assignment group', data = itTicketOthersDF, color = 'blue',order = descending_order)
ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = "right")
plt.tight_layout()
plt.show()

print("The maximum among other groups other than group 0 is: {0}".format(itTicketOthersDF['Assignment group'].value_counts().max()))

# Treat the imbalnce in the itTicketDF dataset by resampling to 661.
df_resampled = df[0:0]
for grp in df['Assignment group'].unique():
    itTicketGrpDF = df[df['Assignment group'] == grp]
    resampled = resample(itTicketGrpDF, replace = True, n_samples = int(661/2), random_state=123)
    df_resampled = df_resampled.append(resampled)

descending_order = df_resampled['Assignment group'].value_counts().sort_values(ascending=False).index
plt.subplots(figsize = (22,5))
ax = sns.countplot(x = 'Assignment group', data = df_resampled, color = 'red')
ax.set_xticklabels(ax.get_xticklabels(), rotation = 45, ha = "right")
plt.tight_layout()
plt.show()

"""### Label Encoding"""

def labelencoder(dataframe) : 
    label_encoder = preprocessing.LabelEncoder() 
    dataframe= label_encoder.fit_transform(dataframe)
    grp_mapping = dict(zip(label_encoder.transform(label_encoder.classes_), label_encoder.classes_))
    return dataframe, grp_mapping

df_resampled['Assignment group'] , mapping_df_resampled = labelencoder(df_resampled['Assignment group'])
df['Assignment group'], mapping_df = labelencoder(df['Assignment group'])

print("Mapping for assignment group in original df: {}".format(mapping_df))

"""### Tf-Idf Vectorizer"""

X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(df['Cleaned Description'], df['Assignment group'], test_size=0.2, random_state=10)
 X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train_df, y_train_df, test_size=0.2, random_state=10)

print('Training set: {0}, {1}\nValidation set: {2}, {3}\nTest set: {4}, {5}\n'.format(X_train_df.shape, y_train_df.shape, X_val_df.shape, y_val_df.shape, X_test_df.shape, y_test_df.shape))

X_train_df = X_train_df.to_frame()
X_test_df = X_test_df.to_frame()
X_val_df = X_val_df.to_frame()

text_transformer = TfidfVectorizer(ngram_range = (1,2), max_features = 15000)
text_transformer
X_train_tfidf = text_transformer.fit_transform(X_train_df['Cleaned Description']).astype('float16')
X_val_tfidf = text_transformer.fit_transform(X_val_df['Cleaned Description']).astype('float16')
X_test_tfidf = text_transformer.fit_transform(X_test_df['Cleaned Description']).astype('float16')

print('Train set shape: {0}\nValidation set shape: {1}\nTest set shape: {2}'.format(X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape))

LSTM_Tfidf_model = Sequential()
LSTM_Tfidf_model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(1, 15000)))
LSTM_Tfidf_model.add(Dropout(0.3))
LSTM_Tfidf_model.add(Bidirectional(LSTM(256, return_sequences=True)))
LSTM_Tfidf_model.add(Dropout(0.25))
LSTM_Tfidf_model.add(Dense(100, activation='relu'))
LSTM_Tfidf_model.add(Flatten())
LSTM_Tfidf_model.add(Dense(50, activation='softmax'))
  

LSTM_Tfidf_model.compile(loss = 'categorical_crossentropy', optimizer = "adam", metrics = ['accuracy'])
LSTM_Tfidf_model.summary()

X_train_tfidf_np = X_train_tfidf.astype(np.float32).A
X_test_tfidf_np = X_test_tfidf.astype(np.float32).A
X_val_tfidf_np = X_val_tfidf.astype(np.float32).A

X_train_tfidf_np = X_train_tfidf_np.reshape(X_train_tfidf_np.shape[0], 1, X_train_tfidf_np.shape[-1])
X_test_tfidf_np = X_test_tfidf_np.reshape(X_test_tfidf_np.shape[0], 1, X_test_tfidf_np.shape[-1])
X_val_tfidf_np = X_val_tfidf_np.reshape(X_val_tfidf_np.shape[0], 1, X_val_tfidf_np.shape[-1])
print('Train set shape: {0}\nValidation set shape: {1}\nTest set shape: {2}'.format(X_train_tfidf_np.shape, X_val_tfidf_np.shape, X_test_tfidf_np.shape))

y_train_df = y_train_df.to_frame()
y_val_df = y_val_df.to_frame()
y_test_df = y_test_df.to_frame()

y_train_onehot = to_categorical(y_train_df, num_classes = 50)
y_val_onehot = to_categorical(y_val_df, num_classes = 50)
y_test_onehot = to_categorical(y_test_df, num_classes = 50)

checkpoint = ModelCheckpoint('/content/drive/MyDrive/NLP/' + 'Tfidf-{epoch:03d}-{val_accuracy:03f}.h5', verbose = 1, monitor = 'val_accuracy', save_best_only = True, mode = 'auto') 
reduceLoss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 2, min_lr = 0.0001)
earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 1)
LSTM_Tfidf_model_history = LSTM_Tfidf_model.fit(X_train_tfidf_np, y_train_onehot, batch_size = 128, epochs = epochs, callbacks = [checkpoint, reduceLoss], validation_data = (X_val_tfidf_np, y_val_onehot))

loss, accuracy = LSTM_Tfidf_model.evaluate(X_test_tfidf_np, y_test_onehot, verbose = 1)
print("Test Loss: {0},\nTest Accuracy: {1}".format(loss, accuracy))

plot_graph(LSTM_Tfidf_model_history)

"""### Word2Vec Embeddings"""

sentences = [line.split(' ') for line in df['Cleaned Description']]
word2vec = Word2Vec(sentences = sentences, min_count = 1)
word2vec.wv.save_word2vec_format('/content/drive/MyDrive/NLP/word2vec_vector.txt')

# load the whole embedding into memory
embeddings_index = dict()
f = open('/content/drive/MyDrive/NLP/word2vec_vector.txt')

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()
print('Loaded %s word vectors.' % len(embeddings_index))

"""### LSTM Model with Word2Vec Embeddings"""

maxlen = 300
numWords = 9000
epochs = 10

def wordTokenizer(dataframe):
    tokenizer = Tokenizer(num_words = numWords, filters = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n', lower = True, split = ' ', char_level = False)
    tokenizer.fit_on_texts(dataframe)
    dataframe = tokenizer.texts_to_sequences(dataframe)
    return tokenizer, dataframe

def tokenizeAndEmbedding(dataframe):
    tokenizer, X = wordTokenizer(dataframe['Cleaned Description'])
    y = np.asarray(dataframe['Assignment group'])
    X = pad_sequences(X, maxlen = maxlen)

    embedding_matrix = np.zeros((numWords+1, 100))
    for i,word in tokenizer.index_word.items():
        if i<numWords+1:
            embedding_vector = embeddings_index.get(word)
            if embedding_vector is not None:
                embedding_matrix[i] = embedding_vector
    print(embedding_matrix.shape)
    return X, y, embedding_matrix

def splitData(X, y):
    print("Number of Samples:", len(X))
    print("Number of Labels: ", len(y))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)
    X_train, X_Val, y_train, y_Val = train_test_split(X, y, test_size=0.2, random_state=10)
    print("Number of train Samples:", len(X_train))
    print("Number of val Samples:", len(X_Val))
    print(X_train[0].shape)
    return X_train, X_test, y_train, y_test, X_Val, y_Val

def fitModel(X_train, y_train, X_Val, y_Val, batch_size, epochs, embedding_matrix, name):
    model = Model()
    input_layer = Input(shape = (maxlen,), dtype = tf.int64)
    print(input_layer.shape)
    embed = Embedding(numWords+1, output_dim = 100, input_length = maxlen, weights = [embedding_matrix], trainable = True)(input_layer)
    lstm = Bidirectional(LSTM(128))(embed)
    drop = Dropout(0.3)(lstm)
    dense = Dense(100, activation = 'relu')(drop)
    out = Dense(len((pd.Series(y_train)).unique()), activation = 'softmax')(dense)   

    model = Model(input_layer, out)
    model.compile(loss = 'sparse_categorical_crossentropy', optimizer = "adam", metrics = ['accuracy'])

    model.summary()

    checkpoint = ModelCheckpoint('/content/drive/MyDrive/NLP/' + name + '-{epoch:03d}-{val_accuracy:03f}.h5', verbose = 1, monitor = 'val_accuracy', save_best_only = True, mode = 'auto') 
    reduceLoss = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 2, min_lr = 0.0001)
    earlyStopping = EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 1)
    model_history = model.fit(X_train, y_train, batch_size = batch_size, epochs = epochs, callbacks = [checkpoint, reduceLoss, earlyStopping], validation_data = (X_Val, y_Val))
    return model_history, model

def train(dataframe, batch_size, epochs, embedding_matrix, name):
    X, y, embedding_matrix = tokenizeAndEmbedding(dataframe)
    X_train, X_test, y_train,y_test, X_Val, y_Val = splitData(X, y)
    model_history, model = fitModel(X_train, y_train, X_Val, y_Val, batch_size, epochs, embedding_matrix, name)
    return model_history, model, X_test, y_test

def plot_graph(model):
    epoch_loss = model.history['loss']
    epoch_val_loss = model.history['val_loss']
    epoch_acc = model.history['accuracy']
    epoch_val_acc = model.history['val_accuracy']

    plt.figure(figsize=(20,6))
    plt.subplot(1,2,1)
    plt.plot(range(0, len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train loss')
    plt.plot(range(0, len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Val loss')
    plt.title('Evolution of LOSS on training and validation sets over epochs')
    plt.legend(loc='best')

    plt.figure(figsize=(20,6))
    plt.subplot(1,2,2)
    plt.plot(range(0, len(epoch_acc)), epoch_acc, 'b-', linewidth=2, label='Train accuracy')
    plt.plot(range(0, len(epoch_val_acc)), epoch_val_acc, 'r-', linewidth=2, label='Val accuracy')
    plt.title('Evolution of ACCURACY on training and validation sets over epochs')
    plt.legend(loc='best')

LSTM_Word2Vec_Raw = Model()
X_test = []
y_test = []
embedding_matrix = []

LSTM_Word2Vec_Raw_history, LSTM_Word2Vec_Raw, X_test, y_test = train(df, 100, epochs, embedding_matrix, 'LSTM-Word2Vec-Raw')

loss, accuracy = LSTM_Word2Vec_Raw.evaluate(X_test, y_test, verbose = 1)
print("Test Loss: {0},\nTest Accuracy: {1}".format(loss, accuracy))

plot_graph(LSTM_Word2Vec_Raw_history)

"""### 30 epochs"""

LSTM_Word2Vec_Raw_30 = Model()
X_test_30 = []
y_test_30 = []
embedding_matrix = []

LSTM_Word2Vec_Raw_30_history, LSTM_Word2Vec_Raw_30, X_test_30, y_test_30 = train(df, 100, 30, embedding_matrix, 'LSTM-Word2Vec-Raw-30epochs')

loss, accuracy = LSTM_Word2Vec_Raw_30.evaluate(X_test_30, y_test_30, verbose = 1)
print("Test Loss: {0},\nTest Accuracy: {1}".format(loss, accuracy))

plot_graph(LSTM_Word2Vec_Raw_30_history)

"""### Resampled data"""

LSTM_Word2Vec_Resampled = Model()
X_test_resampled = []
y_test_resampled = []
embedding_matrix_resampled = []

LSTM_Word2Vec_Resampled_history, LSTM_Word2Vec_Resampled, X_test_resampled, y_test_resampled = train(df_resampled, 100, 10, embedding_matrix_resampled, 'LSTM_Word2Vec_Resampled')

loss, accuracy = LSTM_Word2Vec_Resampled.evaluate(X_test_resampled, y_test_resampled, verbose = 1)
print("For resampled data: \nTest Loss: {0},\nTest Accuracy: {1}".format(loss, accuracy))

plot_graph(LSTM_Word2Vec_Resampled_history)

"""### Resampled - 30 epochs"""

LSTM_Word2Vec_Resampled_30 = Model()
X_test_resampled_30 = []
y_test_resampled_30 = []
embedding_matrix_resampled_30 = []

LSTM_Word2Vec_Resampled_30_history, LSTM_Word2Vec_Resampled_30, X_test_resampled_30, y_test_resampled_30 = train(df_resampled, 100, 30, embedding_matrix_resampled_30, 'LSTM-Word2Vec-Resampled-30epochs')

loss, accuracy = LSTM_Word2Vec_Resampled_30.evaluate(X_test_resampled_30, y_test_resampled_30, verbose = 1)
print("Test Loss: {0},\nTest Accuracy: {1}".format(loss, accuracy))

plot_graph(LSTM_Word2Vec_Resampled_30_history)

df

